\documentclass[a4paper, UKenglish, cleveref]{lipics-v2019}
%\documentclass[a4paper, UKenglish, cleveref]{lipics-v2019}

\bibliographystyle{plainurl}

\title{New minimal linear inferences in Boolean logic independent of switch and medial}%TODO different title?



\author{Anupam Das}{University of Birmingham, United Kingdom}{a.das@bham.ac.uk}{}{}
\author{Alex A. Rice}{University of Cambridge, United Kingdom}{alex.rice@cl.cam.ac.uk}{https://orcid.org/0000-0002-2698-5122}{}

\authorrunning{Anupam Das and Alex Rice}

\Copyright{Copyright - todo}%TODO

\ccsdesc[100]{}%TODO

\keywords{rewriting, linear inference, proof theory, linear logic, implementation}

\nolinenumbers


\usepackage[obeyDraft]{todonotes}
\usepackage{ebproof}

\input{lin-inf-macros}
\input{webmacros}

\usepackage{cmll}
\usepackage{nicefrac}
\usepackage{hyperref}

\begin{document}
\maketitle

\begin{abstract}
A \emph{linear inference} is a valid inequality of Boolean Algebra in which each variable occurs at most once on each side.
Equivalently, it is a linear rewrite rule on Boolean terms that constitutes a true implication.
Linear inferences have played a significant role in structural proof theory, in particular in models of \emph{substructural logics} and in normalisation arguments for \emph{deep inference} proof systems.

Systems of linear logic and, later, deep inference have been founded upon two particular linear inferences, \emph{switch} : $x \land (y \lor z) \to (x \land y) \lor z$, and \emph{medial} : $(w \land x) \lor (y \land z) \to (w \lor y) \land (x \lor z)$.
It is well-known that these two are not enough to derive all linear inferences (even modulo all valid linear equations), but
beyond this little more is known about the structure of linear inferences in general.
In particular despite recurring attention in the literature, it was not known what is the smallest linear inference not derivable under switch and medial.

%A linear inference is a true implication A -> B of boolean logic, where A and B are linear, i.e. each variable occurs at most once in each of A and B. Two important  such implications are switch : x ∧ (y ∨ z) → (x ∧ y) ∨ z, and medial: (w ∧ x) ∨ (y ∧ z) → (w ∨ y) ∧ (x ∨ z).
%Many linear inferences can be derived by composing instances of switch and medial by rewriting. However it has been shown that there are linear inferences that are not derivable in this way, but it was not known if the linear inferences found were minimal.

In this work we leverage recently developed \emph{graphical} representations of linear formulae to build an implementation that is capable of more efficiently searching for switch/medial independent inferences. We use it to find two `minimal' 8-variable independent inferences and also prove that no smaller ones exist; in contrast, a previous approach based directly on formulae reached computational limits already at 7 variables. One of these new inferences derives some previously found independent linear inferences. The other exhibits structure seemingly beyond the scope of previous approaches we are aware of; in particular, its existence contradicts a conjecture of Das and Strassburger.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
A \emph{linear inference} is a true implication $\phi \infers \psi$ of Boolean logic, where $\phi$ and $\psi$ are \emph{linear}, i.e.\ each variable occurs at most once in each of $\phi$ and $\psi$.
Such implications have played a crucial role in many areas of structural proof theory.
For instance the inference \emph{switch},
\[
\s \ : \
x \land (y \lor z)
\ \infers \
(x \land y) \lor z
\]
governs the logical behaviour of the \emph{multiplicative} connectives $\otimes$ and $\parr$ of linear logic \cite{Gir87:linear-logic},
and similarly the inference \emph{medial},
\[
\m \ : \
(w \land x) \lor (y \land z)
\ \infers \
(w \lor y) \land (x \lor z)
\]
governs the logical behaviour of the \emph{additive} connectives $\oplus$ and $\&$ \cite{Str02:lin-log-loc-sys,Str03:phd-thesis}.
Both of these inferences are fundamental to \emph{deep inference} proof theory, in particular allowing structural rules weakening and contraction to be reduced to atomic form \cite{BruTiu01:class-log-loc-sys,Bru03:phd-thesis}, thereby admitting elegant `geometric' proof normalisation procedures based on \emph{atomic flows} \cite{GugGun08:norm-contr-di-at-fl,Gun09:phd-thesis}.
One particular feature of these normalisation procedures is that they are robust under the addition of further linear inferences to the system, thanks to the atomisation of structural steps.

On the other hand the set of \emph{all} linear inferences $\Lin$ plays an essential role in {models} of linear logic and related substructural logics.
In particular, the multiplicative fragment of Blass' \emph{game semantics} model of linear logic validates \emph{just} the linear inferences (there called `binary tautologies') \cite{Bla92:game-semantics-ll}, and this coincides too with the multiplicative fragment of Japaridze's \emph{computability logic}, cf., e.g., \cite{Jap05:intro-to-cirquent-calc}.
From a complexity theoretic point of view, the set $\Lin$ is sufficiently rich to encode all of Boolean logic: it is $\coNP$-complete \cite{Str12:ext-wo-cut,DasStr16:no-compl-lin-sys}.

It was recently shown by one of the authors, together with Strassburger, that, despite its significance, $\Lin$ admits no feasible\footnote{By `feasible', in this work, we always mean polynomial-time computable. This is a natural condition arising from proof theory \cite{CooRec74:length-of-proofs,CooRec79:rel-eff-pps}, and is also required for the result to be meaningful: it prevents us just taking the entire set $\Lin$ as an axiomatisation.} axiomatisation by linear inferences unless $\coNP = \NP$ \cite{DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys}, solving a long-standing open problem of Blass and Japaridze for their respective logics (see, e.g., \cite{Jap17:elem-base-cirq-calc}).
From a Boolean algebra point of view, this means that the class of linear Boolean inequalities has no feasible basis (modulo $\coNP \neq \NP$).
From a proof theoretic point of view this means that any propositional proof system (in the Cook-Reckhow sense \cite{CooRec74:length-of-proofs,CooRec79:rel-eff-pps}, see also \cite{Kra19:cook-reckhow}) must necessarily admit some `structural' behaviour, even when restricted to proving only linear inferences (again, modulo $\coNP\neq \NP$).

An immediate consequence of this result is that $\s$ and $\m$ above do not suffice to generate all linear inferences, even modulo all valid linear equations.\footnote{The valid linear equations are just associativity, commutativity, and unit laws, cf.~\cite{DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys}.}
In fact, this was known before the aforementioned result, due to the identification of an explicit 36 variable inference in \cite{Str12:ext-wo-cut}.\footnote{Strassburger refers to the inference as a `balanced tautology', but like the `binary tautologies' of Blass and Japaridze, these are equivalent to linear inferences. In particular we recast Strassburger's example as a bona fide linear inference in \cref{sec:prev-lin-infs}.}
Already in that work the question was posed whether such an inference was minimal, and since then the identification of a minimal $\{\s,\m \}$-independent linear inference has been a recurring theme in the literature of this area.

It has been verified in \cite{Das13:lin-inf-rew} that a minimal $\{\s,\m \}$-independent linear inference must be `non-trivial', as long as we admit all true linear equations.
Intuitively, `non-triviality' rules out pathological inferences such as $x \land y \infers x \lor y$ or $x \land (y \lor z) \infers x \lor (y \land z)$. For these inferences the variable, say, $y$ is, in a sense, redundant; it turns out that they may be derived in $\{\s,\m\}$, modulo linear equations, from a smaller non-trivial `core'.
We recall these arguments in \cref{sec:preliminaries}.


At the same time \cite{Das13:lin-inf-rew} identified a 10 variable inference that is not derivable by switch and medial (even under linear equations), which Strassburger conjectured was minimal \cite{Str12:private-conjecture}.
Around the same time Sipraga attempted a computational approach, searching for independent linear inferences by brute force \cite{Sip12:aut-search-lin-inf}.
However, computational limits were reached already at 7 variables.
In particular, every linear inference of up to 6 variables is already derivable by switch and medial, modulo linear equations; due to the aforementioned 10 variable inference, any minimal independent linear inference must have size 7,8,9, or 10.


Since 2013 there have been significant advances in the area, in particular through the proliferation of \emph{graph-theoretic} tools.
Indeed, the interplay between formulae and graphs was heavily exploited for the aforementioned result of \cite{DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys}.
Since then, multiple works have emerged in the world of linear proof theory that treat these graphs as `first class citizens', comprising a now active area of research \cite{NguSei18:coh-int-graphs,AccHorStr20:mll-graphs-short,AccHorStr20:mll-graphs-full,CalDasWar20:bgl}.


\paragraph*{Contribution}
In this work we revisit the question of minimal $\{\s,\m\}$-independent linear inferences by exploiting the aforementioned recent graph theoretic techniques.
Such an approach vastly reduces the computational resources necessary and, in particular, we are able to provide a conclusive result:
the smallest $\{\s,\m \}$-independent linear inference has size 8. In fact there are two minimal such ones:\footnote{Minimal with respect to inter-derivability; unique up to associativity, commutativity, renaming of variables and De Morgan duality.}
\begin{equation}
  \label{eq:php32-derived-inf}
  \begin{alignedat}{2}
    & &&(z \lor (w \land w')) \land ((x \land x' ) \lor ((y \lor y') \land z'))\\
    & \to &\quad&(z \land (x \lor y)) \lor ((w \lor y') \land ((w' \land x')  \lor z') )
  \end{alignedat}
\end{equation}
\vspace{-5pt}% Make spacing look ok
\begin{equation}
  \label{eq:counterexample-inference}
  \begin{alignedat}{2}
    & &&((w \land w') \lor (x \land x')) \land ((y \land y') \lor (z \land z'))\\
    & \to&\quad& (w \land y) \lor ((x \lor (w'\land z')) \land ((x'\land y') \lor z) )
  \end{alignedat}
\end{equation}
We dedicate some discussion to each of these separately in \cref{sec:two-found-inferences}, and include a manual verification of their soundness and $\{\s,\m \}$-independence in \cref{sect:app:further-proofs-examples}, as a sanity check.

Our main contribution is an implementation that checks inference for \(\{\s,\m\}\)-derivability, which was able to confirm that all 7 variable linear inference are derivable from switch and medial. In fact we found \eqref{eq:php32-derived-inf} independently of the implementation presented in this paper\footnote{These developments was respectively communicated via blog posts \cite{Ric20:lin-inf-size-7} and \cite{Das20:lin-inf-size-8}.}.
Ultimately, we improved the implementation to run on inferences of size 8 too,
and our inference \eqref{eq:php32-derived-inf} was duly found, as well as \eqref{eq:counterexample-inference} above and its dual.
One highlight of this find is that it exhibits a peculiar structural property that \emph{refutes} Conjecture 7.9 from \cite{DasStr16:no-compl-lin-sys}, as we explain in \cref{sec:counterexample-inference}.

Our implementation is split into a \emph{library} and an \emph{executable}, where the executable implements our search algorithm described in \cref{sec:search}, and the library contains foundations for working with linear inferences using the graph theoretic techniques presented in \cref{sec:webs}. These are written in Rust and designed to be relatively fast while maintaining readability.
Our intention is that this could form a reusable base for future investigations in the area, both for linear formulae and for the recent linear graph theoretic settings of \cite{NguSei18:coh-int-graphs,AccHorStr20:mll-graphs-short,AccHorStr20:mll-graphs-full,CalDasWar20:bgl}.

%\paragraph*{Outline}
%The remainder of this paper is structured as follows.
%In \cref{sec:preliminaries} we present some preliminaries on linear formulae and linear inferences, in particular giving our underlying equational theory and recalling an argument of \cite{Das13:lin-inf-rew} that rules out consideration of `trivial' inferences.
%In \cref{sec:8var-inf} we give a self-contained presentation of our new 8-variable linear inferences and discuss their structural properties.
%In \cref{sec:webs} we recall the graphical treatment of linear inferences from \cite{DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys} that underlies our eventual implementation.
%In \cref{sec:algorithm} we present our implementation, surveying both the library of general tools and the search algorithm for finding new linear inferences~\cite{Ric21:implementation}.
%Finally we give some concluding remarks in \cref{sec:conclusions}.



\section{Preliminaries}
\label{sec:preliminaries}

Throughout this paper we shall work with a countably infinite set of \textbf{variables}, written $x,y, z$ etc.
A \textbf{linear formula} on a (finite) set of variables \(\V\) is built recursively as follows:
\begin{itemize}
\item \(\top\) and \(\bot\) are linear formulae on $\emptyset$, the empty set of variables (called \textbf{units} or \textbf{constants}).
\item \(x\) and \(\neg x\) are linear formulae on \(\{x\}\).\footnote{Note that the restriction of negation to only variables does not compromise expressivity, since the De Morgan laws preserve linearity on a set of variables.}
\item If \(\phi\) is a linear formula on \(\V_1\) and \(\psi\) is a linear formula on \(\V_2\), with \(\V_1 \cap \V_2  = \emptyset \), then \(\phi \lor \psi\) and \(\phi \land \psi\) are linear formulae on \(\V_1 \cup \V_2\).
\end{itemize}
A linear formula that does not contain \(\top\) or \(\bot\) is \textbf{constant-free}.
A linear formula with no negated variables (i.e.\ formulas of form $\neg x$) is \textbf{negation-free}. Later in the paper, we will be able to restrict our search to inferences between constant-free negation-free formulae.

In what follows, we shall omit explicit consideration of variable sets, assuming that they are disjoint whenever required by the notation being used.

A relation \(\sim\) on linear formulae is \textbf{closed under contexts} if for all \(\phi,\psi,\chi\), we have:
\begin{alignat*}{2}
  &\phi \sim \psi \implies \phi \land \chi \sim \psi \land \chi &\quad\quad&\phi \sim \psi \implies \phi \lor \chi \sim \psi \lor \chi\\
  &\phi \sim \psi \implies \chi \land \phi \sim \chi \land \psi &&\phi \sim \psi \implies \chi \lor \phi \sim \chi \lor \psi
\end{alignat*}
An equivalence relation (on linear formulae) that is closed under contexts is called a (linear) \textbf{congruence}.

\begin{definition}
[Linear equations]
Let \(\sim_\ac\) be the smallest congruence satisfying,
\begin{alignat*}{2}
  &\phi \lor \psi \sim_\ac \psi \lor \phi &\quad\quad&\phi \land (\psi \land \chi) \sim_\ac (\phi \land \psi) \land \chi\\
  &\phi \land \psi \sim_\ac \psi \land \phi &&\phi \lor (\psi \lor \chi) \sim_\ac (\phi \land \psi) \land \chi
\end{alignat*}

$\sim_\un$ is the smallest congruence satisfying:
\begin{equation}
  \label{eq:un-eq-rel}
  \begin{alignedat}{4}
    &\phi \land \top \sim_\un \phi   &\quad& \phi \lor \bot\sim_\un \phi &\quad&
    \top \land \phi \sim_\un  \phi  &\quad&  \bot \lor \phi \sim_\un \phi  \\
    &\phi \land \bot \sim_\un \bot  && \phi \lor \top \sim_\un \top  &&
    \bot \land \phi \sim_\un \bot  && \top \lor \phi \sim_\un \top
  \end{alignedat}
\end{equation}

\(\sim_\acu\) is the smallest congruence containing both $\sim_\ac$ and $\sim_\un$.
\end{definition}
Note that we can have \(\phi \sim_\un \psi\) even when \(\phi\) and \(\psi\) have different sets of variables.
Moreover, $\sim_\un$ generates a unique normal form of linear formulae by maximally eliminating constants:

\begin{proposition}
[Folklore, e.g.\ \cite{Das11:bdd-di}]
  \label{prop:unit-free}
  Every formula is $\sim_\un$-equivalent to a unique constant-free formula, or is equivalent to $\bot$ or $\top$.
\end{proposition}

\begin{remark}
[On logical equivalence]
\label{acu-and-logical-equivalence}
Clearly, if $\phi \sim_\acu \psi$ then $\phi$ and $\psi$ are logically equivalent.
In fact, for linear formulae, we also have a converse: two linear formulae $\phi$ and $\psi$ are logically equivalent if and only if $\phi \sim_\acu \psi$ \cite{DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys}.
This property follows from \cref{prop:unit-free} above, the results of \cref{sec:trivial}, and the graphical representation of linear formulae and their semantics in \cref{sec:webs}.
\end{remark}

\subsection{Linear inferences}
A \textbf{linear inference}
%on a set of variables $\V$
is just a valid implication $\phi \to \psi$ (with respect to usual Boolean semantics) where $\phi$ and $\psi$ are linear formulae.
%on sets of variables $\V_1$ and $\V_2$, respectively, with $\V_1 \cup \V_2 = \V$.
The left-hand side (LHS) and right-hand side (RHS) of a linear inference, generally speaking, need not be linear formulae on the same variables.
%Now suppose that \(\phi\) and \(\psi\) are linear formulae. The linear inference \(\phi \to \psi\) is valid if it is a tautology when interpreted in standard (non-linear) propositional logic.
Nonetheless we shall occasionally refer to linear inferences ``on $\V$'' or ``on \(n\) variables'', assuming that the LHS and RHS are both linear formulae on some fixed \(\V\) with \(|\V| = n\).


There are two linear inferences we shall particularly focus on, due to their general prevalence in structural proof theory. \textbf{Switch} is the following inference on 3 variables,
\begin{equation}\label{eq:switch}
\s : x \land (y \lor z) \to (x \land y) \lor z
\end{equation}
and \textbf{medial} is the following inference on 4 variables:
\begin{equation}\label{eq:medial}
\m : (w \land x) \lor (y \land z) \to (w \lor y) \land (x \lor z)
\end{equation}

We may compose switch and medial (and more generally an arbitrary set of linear inferences) to form new linear inferences by construing them as \emph{term rewriting rules}.
More generally, we will consider rewriting derivations modulo the equivalence relations $\sim_\ac$ and $\sim_\acu$ we introduced earlier.
In the latter case, as previously mentioned, the underlying set of variables may change during a derivation, though \cref{prop:unit-free} will later allow us to work with some fixed set of variables throughout $\{\s, \m\}$ derivations.

\begin{definition}[Rewriting]
We write $\to_\s$ and $\to_\m$ for the term rewrite systems generated by \eqref{eq:switch} and \eqref{eq:medial} respectively.
I.e.\ $\to_\s$ and $\to_\m$ are the smallest relations satisfying \eqref{eq:switch} and \eqref{eq:medial}, respectively, closed under substitution\footnote{Again, we assume the substitution respects linearity when required by the context.} and contexts. Write \(\phi \rightsquigarrow_\m \psi\) if \(\phi \sim_\ac \phi' \to_\m \psi' \sim_\ac \psi\) and \(\phi \rightsquigarrow_{\m\un} \psi\) for the same with \(\sim_\ac\) replaced by \(\sim_\acu\). Define \(\rightsquigarrow_\s\), \(\rightsquigarrow_{\s\un}\), \(\rightsquigarrow_{\ms}\), \(\rightsquigarrow_\msu\) similarly.

We write $\phi \redms \psi$ for the reflexive transitive closure of \(\rightsquigarrow_\ms\), and say $\phi \to \psi$ is \textbf{$\{\s,\m\}$-derivable} if \(\phi \redms \psi\).
%
%When the equivalence relation is given by associativity and commutativity (\(\sim_\ac\)), this will be written as \( \phi \redms \psi\), and we will say that the linear inference \(\phi \to \psi\) is derivable using switch and medial.
We may similarly write \(\phi \reds \psi\) (or \(\phi \redm \psi\)), saying $\phi \to \psi$ is $\{\s\}$-derivable (resp., $\{\m\}$-derivable), and similarly for other sets of linear inferences.

Finally, we also write \(\phi \redmsu \psi\), and say that $\phi\to\psi$ is $\{\s,\m \}$\textbf{-derivable with units} in this case.
%When the equivalence relation instead also includes units (\(\sim_\acu\)), we shall write and say that the linear inference \(\phi \to \psi\) is derivable from switch, medial, and units.
Similarly for $\redsu$, $\redmu$ and other sets of linear inferences.
\end{definition}

%The following proposition justifies saying that a linear inference is derivable from switch and medial.
%
%\begin{proposition}[Soundness of rewriting]
%If \(\phi\) and \(\psi\) are linear formulae on a set \(\V\), and \(\phi \redmsu \psi\), then \(\phi \to \psi\) is a valid linear inference.
%\end{proposition}
%\begin{proof}
%  Checking that switch and medial are valid inferences by truth table along with \cref{lem:acu-equiv} and a simple induction gives the result.
%\end{proof}

Clearly, $\s$ and $\m$ are \emph{valid}, so any derivation $\phi \redmsu \psi$ comprises a linear inference.


\begin{example}
[`Mix']
\label{ex:mix}
%  For any \(\phi\) and \(\psi\), the inference
Units can help us derive even constant-free linear inferences.
For instance, \textbf{mix}: \(\phi \land \psi \to \phi \lor \psi\) is $\{\s,\m\}$-derivable with units:
  \[\phi \land \psi \sim_\acu \phi \land (\bot \lor \psi) \to_\s (\phi \land \bot) \lor \psi \sim_\acu \psi \land (\top \lor \phi) \to_\s (\psi \land \top) \lor \phi \sim_\acu \phi \lor \psi\]
  Note that mix is not derivable without using instances of $\sim_\un$.
\end{example}

\begin{example}
[Weakening and duality]
\label{example-weakening-duality}
By setting $\phi = \top$ and $\psi = \bot$ in \cref{ex:mix}, we have:
\[
\bot \sim_\acu \top \land \bot \redmsu \top \lor \bot \sim_\acu \top
\]
Using this we may $\{\s,\m\}$-derive \textbf{weakening}, $\phi \to \phi \lor \chi$, with units as follows:
\[
\phi \sim_\acu \phi \lor (\bot \land \chi) \redmsu \phi \lor (\top \land \chi) \sim_\acu \phi \lor \chi
\]
Notice that $\redmsu$ is closed under De Morgan duality:
If $\phi \redmsu \chi$ and $\bar \phi$ and $\bar \chi$ are obtained from $\phi$ and $\chi$, respectively, by flipping each $\lor$ to a $\land$ and vice versa, then $\bar \chi\redmsu \bar \phi$.
This follows by direct inspection of $\s$, $\m$ and each clause of $\sim_\acu$; indeed the same property holds for $\redms$ by the same reasoning.
%
As a result, we also have that \textbf{coweakening}, $\phi \land \chi \to \phi$, is $\{\s,\m\}$-derivable with units.
\end{example}

We are now able to state the main theorem of this paper:
\begin{theorem}
  \label{thm:main}
  Suppose \(\phi\) is a linear formula over \(\V_1\) and \(\psi\) is a linear formula over \(\V_2\) and \(\mathsf r : \phi \to \psi\) is a linear inference. Then if \(|\V_1 \cap \V_2| \leq 7\) we have that \(\phi \redmsu \psi\).

  Furthermore, there is a valid linear inference \(\phi \to \psi\) on \(8\) variables with \(\phi \not \redmsu \psi\), so \(7\) is maximal with the property above.
\end{theorem}

\subsection{Trivial inferences}
\label{sec:trivial}
In order to state \cref{thm:main} above in its most general form, we have allowed linear formulae to include constants and negation, and linear inferences to be between formulae with different variable sets.
However
it turns out that we may proceed to prove \cref{thm:main}, without loss of generality, by working with constant-free, negation-free formulae on some fixed set of variables, as was already shown in \cite{Das13:lin-inf-rew}. This is done by defining the notion of a \emph{trivial} inference, whose $\{\s,\m \}$-derivability, with units, may be reduced to that of a smaller non-trivial inference.

\begin{definition}
  An inference \(\phi \to \psi\) is \textbf{trivial at a variable \(x\)} if \(\phi[\top/x] \to \psi[\bot/x]\) is again a valid inference.
  An inference is \textbf{trivial} if it is trivial at one of its variables.
\end{definition}


\begin{example}
  The mix inference from \cref{ex:mix}, \(x \land y \to x \lor y\), is trivial at \(x\) and trivial at \(y\).
  Note, however, that it is not trivial at $x$ and $y$ `at the same time', in the sense that the simultaneous substition of $\bot $ for $x$ and $y$ in the LHS and $\top$ for $x$ and $y$ in the RHS does not result in a sound implication.
	In contrast, the linear inference $w \land (x \lor y) \to w \lor (x \land y)$ from \cite{Das13:lin-inf-rew} is, indeed, trivial at $x$ and $y$ {`at the same time'}.

  Neither switch nor medial are trivial.
\end{example}


%\begin{proposition}\label{prop:trivial-comp}
%  Suppose \(\phi \to \psi \to \chi\) are two valid linear inferences. Then if \(\phi \to \psi\) is trivial at \(x\) then \(\phi \to \chi\) is linear at \(x\). Similarly, if \(\psi \to \chi\) is trivial at \(x\) then \(\phi \to \chi\) is again trivial at \(x\).
%\end{proposition}
%\begin{proof}
%  We prove the first one. Since \(\psi \to \chi\) is a valid inference, we have \({\psi[x\mapsto\bot]} \to {\chi[x\mapsto\bot]}\) is a valid inference, and so \({\phi[x\mapsto\top]} \to {\psi[x \mapsto \bot]} \to {\chi[x \mapsto \bot]}\) is a valid inference.
%\end{proof}


\begin{remark}
[Global vs local triviality]
\label{trivial-composition}
Note that triviality is closed under composition by linear inferences: if $\phi \to \psi$ is trivial at $x$ and $\psi \to \chi$ is valid, then $\phi \to \chi$ is trivial at $x$.
Similarly for $\chi \to \psi$ if $\chi \to \phi$ is valid.
One pertinent feature that we shall have to address is that the converse does not hold: there are `globally' trivial derivations that are nowhere `locally' trivial.
For instance consider the following derivation:
\[
w \land x \land (y \lor z)
\rightsquigarrow_\s
w \land ((x \land y) \lor z)
\rightsquigarrow_\s
(w\land z) \lor (x \land y)
\rightsquigarrow_\m
(w\lor x) \land (y \lor z)
\]
The derived inference is just an instance of mix, from \cref{ex:mix}, on the redex $w \land x$, which is trivial.
However, no local step is trivial.
\end{remark}

To prove (the first half of) \cref{thm:main}, in \cref{sec:algorithm} we will actually prove the following apparent weakening of that statement:

\begin{theorem}
  \label{thm:main-reduced}
  Let \(n < 8\). Let \(\phi\) and \(\psi\) be constant-free negation-free linear formulae on \(n\) variables and suppose \(\phi \to \psi\) is a non-trivial linear inference. Then \(\phi \redms \psi\).
\end{theorem}
In fact this statement is no weaker at all, and we will now see how the consideration of triviality allows us to only deal with such special cases without loss of generality.

\begin{proposition}
[\cite{Das13:lin-inf-rew}, Theorem 34]
  \label{prop:trivial}
  Let $\phi$ and $\psi$ be linear formulae on $\V_1$ and $\V_2$, respectively,
  and let \(\mathsf r : \phi \to \psi\) be a linear inference.
  There is a non-trivial linear inference \(\mathsf r' : \phi' \to \psi'\) on some $\V' \subseteq \V_1 \cap \V_2$ such that $\mathsf r : \phi \to \psi$ is $\{\s,\m,\mathsf r' \}$-derivable with units.
\end{proposition}
%\begin{proof}
%  Implied by \cite[Lemma 34]{Das13:lin-inf-rew}.
%\end{proof}

Note in particular that, in the statement above, if $\mathsf r'$ is $\{\s,\m \}$-derivable with units, then so is $\mathsf r$.
This is also the case for the next result.

\begin{proposition}
  \label{prop:const-free-neg-free}
  Let \(\mathsf r: \phi \to \psi\) be a non-trivial linear inference among variables $\V \neq \emptyset$.
  Then there is a constant-free negation-free non-trivial linear inference \(\mathsf r' : \phi' \to \psi'\) on $\V$
  s.t.\ $\mathsf r: \phi \to \psi$ is $\{\s,\m,\mathsf r' \}$-derivable with units.
%  \(\phi \redmsu \psi\) if and only if \(\phi' \redmsu \psi'\) and \(\phi'\) and \(\psi'\) have the same variables.
\end{proposition}
\begin{proof}
  First, note that both $\phi$ and $\psi$ must be linear formulae on $\V$, since $\phi \to \psi$ is non-trivial.
  For the same reason, no variable can occur positively in $\phi$ and negatively in $\psi$ or vice-versa, since $\phi \to \psi$ is non-trivial, and so any negated variable may be safely replaced by its positive counterpart.
%  Further, by substituting variables by their negation in both formulae, which preserves validity, we can assume \(\phi\) is negation free.
From here, we simply set $\phi'$ and $\psi' $ to be the constant-free formulae (uniquely) obtained from \cref{prop:unit-free} by $\sim_\un$.
Non-triviality of $\mathsf r'$ follows from that of $\mathsf r$ by logical equivalence.
%
%  Now if \(\phi'\) and \(\psi'\) have different sets of variables, the inference will be trivial, and so \(\phi'\) and \(\psi'\) must have the same set of variables. Similarly if \(\psi'\) contains a negated variable \(x\), then \(\psi'[x \mapsto \top]\) implies \(\psi'[x \mapsto \bot]\) and so \(\phi'[x \mapsto \top]\) implies \(\psi'[x \mapsto \bot]\) which would make the implication trivial at \(x\). Therefore \(\psi'\) is negation free.
\end{proof}

\begin{corollary}\label{cor:main-red-to-main}
  The statement of \cref{thm:main-reduced} implies (the first half of) the statement of \cref{thm:main}.
\end{corollary}
\begin{proof}
Let $\mathsf r$ be as in \cref{thm:main}.
Let $\mathsf r'$ be the non-trivial linear inference obtained by \cref{prop:trivial} above, and let $\mathsf r''$ be the non-trivial constant-free negation-free linear inference thence obtained by \cref{prop:const-free-neg-free}.
By \cref{thm:main-reduced} $\mathsf r''$ is $\{\s,\m \}$-derivable and so, by \cref{prop:trivial} and \cref{prop:const-free-neg-free}, $\mathsf r$ is also $\{\s,\m \}$-derivable with units.
\end{proof}
%\begin{proof}
%  It is sufficient to prove that for all \(n < 7\) that if all inferences on \(n\) or fewer variables are derivable then all inferences on \(n+1\) variables are derivable.
%
%  Therefore suppose all inferences of \(n\) or fewer variables are derivable and assume we have \(\phi \to \psi\) with \(n+1\) variables. If the inference is trivial then we are done using \cref{thm:trivial} and the assumption. Otherwise there is \(\phi' \to \psi'\) which is constant-free and negation-free. Then by \cref{thm:main-reduced} we have \(\phi' \redms \psi'\) and so \(\phi' \redmsu \psi'\) which implies that \(\phi \redmsu \psi\) as required.
%\end{proof}

It is clear that if an inference is derivable with switch and medial then it is also derivable with switch, medial, and units. The following proposition, while not necessary for the proof of \cref{cor:main-red-to-main}, allows the other direction in some cases, and is the reason why our search algorithm in \cref{sec:algorithm} will only check for $\{\s,\m\}$-derivability.

\begin{proposition}
[Follows from \cite{Das13:lin-inf-rew}, Lemma 28]
\label{non-triv-const-free-neg-free-derivability-without-units}
  Suppose \(\phi \to \psi \) is a non-trivial constant-free negation-free linear inference that is $\{\s,\m \}$-derivable with units. Then \(\phi \to \psi\) is also $\{\s,\m \}$-derivable (without units).
\end{proposition}
The idea here is to systematically rewrite a derivation with units to one without, line by line under \cref{prop:const-free-neg-free}.
Crucially, the invariant of non-triviality constrains the contexts in which constants may occur, ensuring that the constant-elimination procedure preserves instances of $\s$ or $\m$.

%\begin{proof}
%  By \cite[Lemma 28]{Das13:lin-inf-rew}, it suffices to show that no variable is syntactically trivialised in the derivation. Without loss of generality we can assume that the only variables appearing in the derivation are those which appear in \(\phi\) or \(\psi\), as any other variable could be replaced throughout by \(\top\) or \(\bot\). Further, none of the original variables can be syntactically trivialised, as then they would be trivial in the inference (by an argument involving \cref{prop:trivial-comp}). Therefore, normalising each line of the derivation (using \cref{lem:unit-free}) with respect to units and removing now constant lines in the derivation produces a derivation without units.
%\end{proof}

\subsection{Minimality of inferences}
Let us take a moment to explain the various notions of `inference minimality' that we shall mention in this work.
% We will often speak of a `X minimal' (linear) inference satisfying a certain property, in this work typically the property of being $\{\s,\m \}$-independent.
% The three variations of this we shall mention are \emph{size minimality}, \emph{logical minimality}, and \emph{\(\{\s,\m\}\)-minimality}.

% In this section, an overview of each of these is given.

\textbf{Size minimality} refers simply to the number of variables the inference contains. E.g.\ when we say that the \(8\)-variable inferences in the next section are size minimal (or `smallest') non-\(\{\s,\m\}\)-derivable with units (or \textbf{$\{\s,\m \}$-independent}) linear inferences, we mean that there are no \(\{\s,\m\}\)-independent linear inferences with fewer variables.

A linear inference \(\phi \to \psi\) is \textbf{logically minimal} if there is no $\sim_\acu$-distinct interpolating linear formula.
  I.e.\ if \(\phi \to \chi\) and \(\chi \to \psi\) are linear inferences, then $\chi$ is $\sim_\acu$-equivalent to $\phi$ or $\psi$ (and so, by \cref{acu-and-logical-equivalence}, is logically equivalent to \(\phi\) or \(\psi\)).


  Finally, a linear inference \(\phi \to \psi\) is \textbf{\(\{\s,\m\}\)-minimal} if there is no formula $\chi$ s.t.\ \(\phi \rightsquigarrow_\ms \chi\) or \(\chi \rightsquigarrow_\ms \psi\) and $\chi \to \psi$ or $\phi \to \chi$, respectively, is a valid linear inference which is not a logical equivalence.


It is clear from the definitions that any logically minimal inference is also \(\{\s,\m\}\)-minimal, though the converse may not be true.
The reason for considering \(\{\s,\m\}\)-minimality is that it is easier to systematically check by hand.
In fact, the implementation we give later in \cref{sec:algorithm} further verifies that our new $8$-variable inferences are logically minimal.
%; checking the \(\s,\m\)-minimality of these inferences simply serves as a  sanity check for our implementation.


Logical minimality also serves an important purpose for our proof of \cref{thm:main-reduced}, as it allows the following reduction, greatly reducing the search space for our implementation, in fact to nearly $1\%$ of its original size for \(8\) variable inferences:\footnote{\(\nicefrac{5364}{514486} \approx 1.04\%\)}

\begin{lemma}\label{lem:minimality}
  Suppose the statement of \cref{thm:main-reduced} holds whenever \(\phi \to \psi\) is logically minimal. Then the statement of \cref{thm:main-reduced} holds (even when \(\phi \to \psi\) is not logically minimal).
\end{lemma}
\begin{proof}
  Suppose we have a non-trivial inference between constant-free negation-free linear inferences \(\phi \to \psi\). Then \(\phi \to \psi\) can be refined into a chain of logically minimal linear inferences \(\phi \to \chi_0 \to \dots \to \chi_n \to \psi\). All of these must be non-trivial (as triviality of any of them would imply triviality of \(\phi \to \psi\)). Therefore if all such inferences are derivable from switch and medial (with units) then so is \(\phi \to \psi\), by transitivity.
\end{proof}

\section{New 8-variable $\{\s,\m\}$-independent linear inferences}
\label{sec:8var-inf}
In this section we shall present the new 8-variable linear inferences of this work (\eqref{eq:php32-derived-inf} and \eqref{eq:counterexample-inference} from the introduction), and give self-contained arguments for their $\{\s,\m\}$-independence and $\{\s,\m\}$-minimality, as a sort of sanity check for the implementation described in the next section.
We shall also briefly discuss some of their structural properties, in reference to previous works in the area.
Thanks to the results of the previous section, in particular \cref{prop:const-free-neg-free} and \cref{trivial-composition}, we shall only consider non-trivial constant-free negation-free linear inferences with the same variables in the LHS and RHS.
Furthermore, by \cref{non-triv-const-free-neg-free-derivability-without-units} we shall only consider $\{\s,\m \}$-derivability (i.e., without units).

\subsection{Previous linear inferences}
\label{sec:prev-lin-infs}
In \cite{Str12:ext-wo-cut} Strassburger presented a 36-variable inference that is $\{\s,\m\}$-independent, by an encoding of the pigeonhole principle with 4 pigeons and 3 holes.
He there referred to it as a `balanced' tautology, but in our setting it is a linear inference that can be written as follows:\footnote{We write Strassburger's inference by encoding each $q_{i1j}$ as $\pp i j $, each $q_{i2j}$ as $\qq i j $, each $q_{i3j}$ as $\rr i j $, and using `primed' variables instead of duals, with the LHS of the inference being the appropriate instances of excluded middle.}
\[
\begin{array}{rc}
& \bigwedge\limits_{i=1}^3 \bigwedge\limits_{j=1}^i (\pp i j \lor \pp i j ' )
\land
\bigwedge\limits_{i=1}^3 \bigwedge\limits_{j=1}^i (\qq i j \lor \qq i j ' )
\land
\bigwedge\limits_{i=1}^3 \bigwedge\limits_{j=1}^i (\rr i j \lor \rr i j ' )
\\
\noalign{\smallskip}
\to &
\left[
\begin{array}{r@{\quad ( (}l@{)\ \land \ (}l@{)\ \land \ (}l@{))}}
	 & \pp 1 1  \lor \pp 2 1  \lor \pp 3 1   & \qq 1 1 \lor \qq 2 1 \lor \qq 3 1  & \rr 1 1 \lor \rr 2 1 \lor \rr 3 1  \\
\lor  & \pp 1 1 ' \lor \pp 2 2 \lor \pp 3 2 & \qq 1 1 ' \lor \qq 2 2 \lor \qq 3 2 & \rr 1 1 ' \lor \rr 2 2 \lor \rr 3 2 \\
\lor & \pp 2 1 ' \lor \pp 2 2 ' \lor \pp 3 3 & \qq 2 1 ' \lor \qq 2 2 ' \lor \qq 3 3 & \rr 2 1 ' \lor \rr 2 2 ' \lor \rr 3 3 \\
\lor  & \pp 3 1 ' \lor \pp 3 2 ' \lor \pp 3 3 ' & \qq 3 1 ' \lor \qq 3 2 ' \lor \qq 3 3 ' & \rr 3 1 ' \lor \rr 3 2 ' \lor \rr 3 3 '
\end{array}
\right]
\end{array}
\]
In \cite{Das13:lin-inf-rew} Das noticed that a more succinct encoding of the pigeonhole principle could be carried out, with only 3 pigeons and 2 holes, resulting in a 10-variable $\{\s,\m \}$-independent linear inference.
A variation of that, e.g.\ as used in \cite{Das17:unavoidable-con-loop}, is the following:
\begin{equation}
\label{eq:10varinf-nonminimal}
\begin{alignedat}{2}
&&& (z \lor (w \land w')) \land (y \lor y') \land (u \lor u') \land ((x \land x') \lor z') \\
&\to &\quad& (z \land (x \lor y)) \lor (u \land x') \lor (w' \land u') \lor ((w \lor y') \land z')
\end{alignedat}
\end{equation}
%
%
%The idea is as follows: \todo{write this}
%
%\begin{tabular}{|c|c|}
%\hline
%$u$ & $v\quad v'$ \\ \hline
%$w \quad w'$ & $x\quad x'$ \\ \hline
%$y \quad y'$ & $z$ \\ \hline
%\end{tabular}
%
In fact this is not a $\{ \s,\m \}$-minimal inference, but we write this one here for comparison to one of the new 8-variable inferences in the next subsection.
%The precise minimal inference presented in [Das13] is:
%
%\todo{write inference}
%
It can be checked valid and non-trivial by simply checking all cases, or by use of a solver.
We do not give an argument for $\{\s,\m \}$-independence here, but such an argument is similar to the one we give for an 8-variable inference \cref{eq:php32-derived-inf-repeated}, which is given the next subsection.


\subsection{The two minimal 8 variable $\{\s,\m\}$-independent linear inferences}
\label{sec:two-found-inferences}
Pre-empting \cref{sec:search}, let us explicitly give the two minimal linear inferences found by our algorithm and justify their $\{\s,\m \}$-independence and $\{\s,\m \}$-minimality, as a sort of sanity check for our implementation later.
As we will see, they both turn out to be significant in their own right, which is why we take the time to consider them separately.


%As we mentioned before, our implementation checked more than just $\{\s,\m \}$-minimality with respect to $\{\s,\m\}$-independence: any interpolating linear formula $X$, i.e.\ with LHS $\implies X \implies $ RHS, is already either the LHS or RHS, up to linear equations.
%We do not verify this property by hand here since the analysis is long and cumbersome.


\subsubsection{A refinement of the 3-2-pigeonhole-principle}
\label{sec:php32-refined}
First let us consider the 8 variable linear inference that may be used to derive \cref{eq:10varinf-nonminimal}, cf.~\cref{sect:reduction-10var-to-8var} (identical to \eqref{eq:php32-derived-inf} from the introduction):% \footnote{Note that, with respect to \cref{eq:10varinf-nonminimal} and the argument in \cref{sect:reduction-10var-to-8var}, we have flipped the disjunction $w\lor y$ to $y \lor w$ in the RHS so that duality is more immediate.}
\begin{equation}
\label{eq:php32-derived-inf-repeated}
\begin{alignedat}{2}
&&& (z \lor (w \land w')) \land ((x \land x') \lor ((y \lor y') \land z')) \\
&\to &\quad& (z \land (x \lor y)) \lor ((w \lor y') \land ((w' \land x') \lor z'))
\end{alignedat}
\end{equation}
Recalling the notion of `duality' from \cref{example-weakening-duality}, let us define the \textbf{dual} of a linear inference $\phi \to \chi$ to be the linear inference $\bar \chi \to \bar \phi$, where $\bar \phi$ and $\bar \chi$ are obtained from $\phi$ and $\chi$, respectively, by flipping all $\lor$s to $\land$s and vice-versa.
Considering linear inferences up to renaming of variables, we have:

\begin{observation}
\label{duality-of-php32-8var}
\eqref{eq:php32-derived-inf-repeated} is self-dual.
\end{observation}
\noindent
Indeed, the formula structure of the RHS is clearly the dual of that of the LHS, and the mapping from a variable in the LHS to the variable at the same position in the RHS is, in fact, an involution. I.e., $u$ is mapped to itself; $v$ is mapped to $y$ which in turn is mapped to $v$; $v'$ is mapped to $w$ which is in turn mapped to $v'$; $x $ is mapped to $y'$ which in turn is mapped to $x$; and $z$ is mapped to itself.
Validity may be routinely checked by any solver, but we give a case analyis of assignments in \cref{sec:app:validity-php32-derived}.

We may also establish $\{\s,\m \}$-independence and $\{\s,\m \}$-minimality by checking all applications of $\s$ or $\m$ to the LHS (note that we do not need to check the RHS, by \cref{duality-of-php32-8var} above).
This analysis is given explcitly in \cref{sec:ind-min-php32}.


%\subsubsection{Inference 1}
%\[
%\begin{array}{rl}
%& (u \lor (v \land v')) \land ((y \land y' ) \lor ((w \lor x) \land z)) \\
%\to & ((u \lor (v'\land y') ) \land (w \lor y) ) \lor ((v \lor x) \land z)
%\end{array}
%\]

\subsubsection{A counterexample to a conjecture of Das and Strassburger}
\label{sec:counterexample-inference}
Finally, our search algorithm found a completely new linear inference
(identical to \eqref{eq:counterexample-inference} from the introduction):
%\[
%\begin{array}{rl}
%& (w \lor y ) \land ((x \land (w' \lor z')) \lor ((x' \lor y') \land z) ) \\
%\to & ((w \lor w') \land (x \lor x' )) \lor ((y \lor y') \land (z \lor z'))
%\end{array}
%\]
\begin{equation}
\label{eq:counterexample-inference-repeated}
\begin{array}{rl}
& ((w \land w') \lor (x \land x')) \land ((y \land y') \lor (z \land z')) \\
\to & (w \land y) \lor ((x \lor (w'\land z')) \land ((x'\land y') \lor z) )
\end{array}
\end{equation}


Again, validity is routine, but a case analysis is given in \cref{sec:app:validity-counterexample-inference}.
We may establish $\{\s,\m \}$-independence and $\{\s,\m \}$-minimality again by checking all possible rule applications.
This analysis is given in \cref{sec:ind-min-counterexample-inference}.


This new inference exhibits a rather interesting property, which we shall frame in terms of the following notion, since it will be critical for the next section:
\begin{definition}
\label{definition-of-lccs}
Let $\phi$ be a linear formula on a variable set $\V$. For distinct $x,y \in \V$, the \textbf{least common connective} (lcc) of $x,y $ in $\phi$ is the connective $\lor$ or $\land$ at the root of the smallest subformula of $\phi$ containing both $x$ and $y$.
\end{definition}
Note that, in the inference \eqref{eq:counterexample-inference-repeated} above, the lcc of $w' $ and $x'$ changes from $\lor$ to $\land$, but the lcc of $y$ and $y'$ changes from $\land$ to $\lor$.
No such example of a minimal linear inference exhibiting both of these properties was known before; switch, medial and all of the linear inferences of this section either preserve $\lor$-lccs or preserve $\land$-lccs.
In fact, Das and Strassburger showed that any valid linear inference preserving $\land$-lccs is already derivable by medial \cite[Theorem 7.5]{DasStr16:no-compl-lin-sys}, and further conjectured that there was no minimal inference that preserves neither $\land$-lccs nor $\lor$-lccs.
Naturally, our new inference is a counterexample to that:

\begin{theorem}
Conjecture 7.9 from \cite{DasStr16:no-compl-lin-sys} is false.
\end{theorem}


\section{A graph-theoretic presentation of linear inferences}
\label{sec:webs}
A significant cause of algorithmic complexity when searching for linear inferences is the multitude of formulae equivalent modulo associativity and commutativity ($\sim_\ac$). For example, for 7 variables, there are \(42577920\) formulae (ignoring units), yet only \(78416\) equivalence classes.
Under \cref{acu-and-logical-equivalence} it would be ideal if we could deal with $\sim_\ac$-equivalence classes directly, realising logical and syntactic notions on them in a natural way.
This is precisely what is accomplished by the graph-theoretic notion of a \emph{relation web}, cf.~ \cite{Gug07:sys-int-struct,Str07:char-med,DasStr15:no-comp-lin-sys,DasStr16:no-compl-lin-sys}.

Throughout this section we work only with constant-free negation-free linear formulae, cf.~\cref{thm:main-reduced}.
Recall the notion of \emph{least common connective} (lcc) from \cref{definition-of-lccs}.

\begin{definition}
Let $\phi$ be a linear formula on a variable set $\V$.
The \textbf{relation web} (or simply \textbf{web}) of $\phi$, written $\W(\phi)$, is a simple undirected graph with:
\begin{itemize}
\item The set of nodes of $\W (\phi)$ is just $\V$, i.e.\ the variables occurring in $\phi$.
\item For $x,y \in \V$, there is an edge between $x$ and $y$ in $\W(\phi)$ if the lcc of $x$ and $y$ in $\phi$ is $\land$.
\end{itemize}
\end{definition}

When we draw graphs, we will draw a solid red line \(\redge[]{x}{y}\) if there is an edge between \(x\) and \(y\), and a green dotted line \(\gedge[]{x}{y}\) otherwise.
\begin{example}\label{ex:relation-web}
  Let $\phi$ be the linear formula \(w \land (x \land (y \lor z))\). \(\W(\phi)\) is the following graph:

\begin{center}
    $ \FourGraph{w,x,y,z}rrrrrg $
\end{center}
\end{example}

  Note that linear formulae equivalent up to associativity and commutativity have the same relation web, since $\sim_\ac$ does not affect the lccs.
  For instance, if \(\psi = (w \land x) \land (z \lor y)\), then $\W(\psi)$ is still just the relation web above.
  In fact, we also have the converse:
  \begin{proposition}
  [E.g., \cite{DasStr16:no-compl-lin-sys}, Proposition 3.5]
    Given linear formulae \(\phi\) and \(\psi\), \(\phi \sim_\ac \psi\) if and only if \(\W(\phi) = \W(\psi)\).
  \end{proposition}
Thus relation webs are natural representations of equivalence classes of linear formulae modulo associativity and commutativity.


It is easy to see that the image of $\W$ is just the \emph{cographs}. A \textbf{cograph} is either a single node, or has the form \(\redge \R \S\) or $\gedge \R \S$ for cographs $\R$ and $\S$.\footnote{Formally, $\redge \R \S$ has as nodes the disjoint union of the nodes of $\R$ and the nodes of $\S$; edges within the $\R$ component are inherited from $\R$ and similarly for $\S$; there is also an edge between every node in $\R$ and every node in $\S$. $\gedge \R \S$ is defined similarly, but without the last clause.}
A \textbf{cograph decomposition} of a cograph $\R$ is just a definition tree according to these construction rules (its `cotree'), from which we may easily extract a linear formula with web $\R$.
Note from \cref{ex:relation-web} that the cograph decomposition of a relation web need not be unique.
%It can be easily seen that these cographs correspond directly to the graphs that can be built from \(\W\).


%We can also talk about a cograph decomposition of a graph, which is a series of disjoint unions and duals of disjoint unions which creates the graph. The following example demonstrates this, as well as showing that a cograph decomposition need not be unique. Finding a cograph decomposition of a graph amounts to the same thing as finding a formula \(\phi\) such that \(\W(\phi)\) is the original graph.



Cographs admit an elegant \emph{local} characterisation by means of forbidden subgraphs:
\begin{definition}
   \(P_4\) is the following graph:
  \begin{center}
    \(\FourGraph{w,x,y,z}rgggrr\)
  \end{center}
  A graph \(G\) is \(P_4\)-free if none of its induced\footnote{An induced subgraph is one whose edges are just those of $G$ restricted to a subset of the nodes.} subgraphs are isomorphic to \(P_4\).
\end{definition}

\begin{proposition}
[E.g.\ \cite{Gug07:sys-int-struct,Str07:char-med}]
  A graph is a cograph if and only if it is \(P_4\)-free.
  Thus, relation webs are just the $P_4$-free graphs whose nodes are variables.
\end{proposition}

Note, in particular, that this characterisation gives us an easy way to check whether a graph is the web of some formula: just check every $4$-tuple of nodes for a $P_4$.
What is more, we may also verify several semantic properties of linear inferences, such as validity and triviality, directly at the level of relation webs:

%Following on from \cref{ex:relation-web}, we in fact we get the following result:

%As linear inference respects associativity and commutativity, such that if \(\phi \to \psi\) is valid and \(\phi \sim_\ac \phi'\) and \(\psi \sim_\ac \psi'\) then \(\phi' \to \psi'\), it makes sense to talk about whether one relation web linear implies another. We would like to be able to do this without having to find cograph decompositions for each graph and then running a sat solver. Luckily there exists a nice criterion for checking the validity of the inference.

\begin{proposition}
[Follows from Proposition 4.4 and Theorem 4.6 in \cite{DasStr16:no-compl-lin-sys}]
  \label{lem:rw-inference}
  Let $\phi$ and $\psi$ be linear formulae on the same set of variables.
   \(\phi \to \psi\) is a valid linear inference if and only if for every maximum clique \(P\) of \(\W(\phi)\), there is some \(Q \subseteq P\) such that \(Q\) is a maximum clique of \(\W(\psi)\).
\end{proposition}

%Further we have a criterion for detecting which of these inferences are trivial.

\begin{proposition}
[E.g., \cite{DasStr16:no-compl-lin-sys}, Proposition 5.7]
  \label{lem:rw-trivial}
  Let $\phi$ and $\psi$ be linear formulae on the same variables.
   \(\phi \to \psi\) is a linear inference that is trivial at \(x\) if and only if for every maximum clique \(P\) of \(\W(\phi)\), there is some \(Q \subseteq P \setminus \{x\}\) such that \(Q\) is a maximum clique of \(\W(\psi)\).
\end{proposition}
\noindent
Note that the criterion for triviality is a strict strengthening of that for validity, as we would expect.
For both of the results above, there is a \emph{dual} characterisation in terms of \emph{maximal stable sets} instead of maximal cliques.
For instance, the characterisation of validity morally states ``whenever $\phi$ evaluates to $1$, then $\psi$ evaluates to $1$''.
The dual characterisation is that for every maximal stable set $Q$ of $\W(\psi)$ there is a maximal stable set $P$ of $\W(\phi)$ with $P\subseteq Q$, and morally states ``whenever $\psi$ evaluates to $0$, then $\phi$ evaluates to $0$''.
We will not make use of these dual characterisations in this work.

\begin{example}
[Validity of switch and medial, triviality of mix]
The switch and medial inferences can be construed as the following graph rewrite rules on relation webs, respectively:
\begin{center}
\(
\s \ : \
\ThreeGraph{x,y,z}rrg
\ \to \
\ThreeGraph{x,y,z}rgg
\qquad\qquad
\m\ : \
\FourGraph{w,x,y,z}rggggr
\ \to \
\FourGraph{w,x,y,z}rgrrgr
\)
\end{center}
It is easy to see that the validity criterion of \cref{lem:rw-inference} holds for each of these rules.
For $\s$, the maximal cliques $\{x,y\}$ and $\{x,z\}$ in the LHS are mapped to $\{x,y\}$ and $\{z\}$ in the RHS respectively.
For $\m$, the maximal cliques $\{w,x\}$ and $\{y,z \}$ in the LHS are mapped to themselves in the RHS.

Now consider the trivial inference $x\land y \to x\lor y$, construed as the graph rewrite rule:
\begin{center}
$\redge x y \ \to \ \gedge x y $
\end{center}
We can easily verify the criterion for triviality at $x$ from \cref{lem:rw-trivial} since the only maximal clique on the LHS, $\{x,y\}$ has $\{y\} \subseteq \{x,y\}\setminus \{x\}$ as a subset on the RHS.
\end{example}

\begin{remark}\label{rem:using-webs}
  With the results in this section, the notation for inferences between formulae can be used for relation webs. For example, for webs \(\R\) and \(\S\), we can write \(\R \to \S\) is valid to mean that the inference between (any choice of) the underlying linear formulae is valid, and \(\R \redms \S\) to mean the there is a derivation from switch and medial between the underlying formulae. These notions are invariant up to associativity and commutativity and therefore are independent of the particular cograph decomposition chosen.

  Furthermore, for proving \cref{thm:main-reduced} it is sufficient to show that for all webs \(\R\) and \(\S\) with size less than \(8\) and non-trivial inference \(\R \to \S\), that \(\R \redms \S\).
\end{remark}

The final component needed to be able to work fully with webs is a way to check if a given inference is an instance of switch or medial. Such characterisations exist:

\begin{proposition} [{\cite[Theorem~5]{Str07:char-med}}]\label{prop:medial-char}
Let $\R \to \S$ represent a constant-free negation-free non-trivial linear inference.
%Write $\R = \W(\phi) $ and $\S = \W(\psi)$.
Then $\R \to \S$ is derivable from medial if and only if:
  \begin{itemize}
  \item Whenever \(\redge x y \) in $\R$, also \(\redge x y\)  in \( \S \).
  \item Whenever \(\gedge x y \) in \(\R \) but \(\redge x y \) in \( \S \) there exists \(w\) and \(z\) such that
    \begin{center}
      \(\FourGraph{w,x,y,z}rggggr\) is an induced subgraph of \(\R \) and \(\FourGraph{w,x,y,z}rgrrgr\) is an induced subgraph of \(\S \).
    \end{center}
  \end{itemize}
\end{proposition}
As we previously mentioned, the second condition can, in fact, be replaced by simply requiring that $\R \to \S$ is valid \cite[Theorem 7.5]{DasStr16:no-compl-lin-sys}.
A relation web characterisation for switch derivability can also be found in \cite[Theorem~6.2]{Str07:char-med}, however we do not use it in our implementation.

\section{Implementation}
\label{sec:algorithm}

As stated in previous sections, \cref{thm:main-reduced} is proven using a computational search. In this section we describe the algorithm used to search for \(\{\s,\m\}\)-independent inferences, as well as some of the optimisations we employ so that this search finishes in a reasonable time.
Many of these optimisations may be of self-contained theoretical interest.

The implementation is written in Rust,\footnote{\url{https://www.rust-lang.org/}} which offers a combination of good performance (both in terms of speed and memory management) but also provides a variety of high level abstractions such as algebraic data types. Furthermore, it has good inbuilt support for iterators allowing the code to be written in a more functional style and has a built-in testing framework, meaning that sanity checks can be built into the code base. The code is available at \cite{Ric21:implementation} and has been split into two parts: a \emph{library} containing types for undirected linear graphs and formulae and some operations on them, and an \emph{executable} which implements the search algorithm using this library as a base.

\subsection{Library}
\label{sec:library}

The library portion of the implementation defines methods for working with relation webs, as well as the ability to convert formulae to relation webs and vice versa. The majority of the library consists of the \texttt{LinGraph} trait, which is an interface for types that can be treated as linear undirected graphs. This allows us to query the edges between variables as well as perform more involved operations such as checking whether a graph is \(P_4\)-free.
We may also ask whether a pair of relation webs forms a valid linear inference and check whether the inference is trivial using \cref{lem:rw-inference,lem:rw-trivial}.


\textbf{Storing graphs and relation webs.}
The library was designed with the intention of storing graphs as compactly as possible.
 Therefore there are implementations of \texttt{LinGraph} which pack the data (a series of bits for whether there exists an edge between each pair of nodes) into various integer types. The implementation is given for unsigned 8 bit, 16 bit, 32 bit (which can store up to 8 variable graphs), 64 bit, and 128 bit integers. Furthermore there is an implementation using vectors (variable length arrays) of Boolean values, which is less memory efficient though can store relation webs of arbitrary size. A further improvement could be to use an external library implementing bit arrays to make a memory efficient, yet infinitely scalable implementation.

\textbf{Checking an inference between graphs.}
In order to implement linear inference checking, we use a data type representing maximal cliques of a relation web, which we represent as the trait \texttt{MClique}. It is possible to use Rust's inbuilt \texttt{HashSet}\footnote{\url{https://doc.rust-lang.org/std/collections/struct.HashSet.html}} to do this, though as above, a more memory efficient solution is provided where we store the data in a single integer, with each bit determining whether a node is contained in the clique. For example a maximal clique on an 8 node graph can be encoded into a single byte. While checking for linear inferences and triviality, the main operation on maximal cliques is asking whether one is a subset of the other. This operation can be carried out very quickly using bitwise operations. Lastly we also need a way to generate the maximal cliques of a relation web. This is done using the Bron-Kerbosch algorithm\cite{BroKer73:finding-all-maxcliques}, which is fast enough for our purposes (as we are only finding the maximal cliques of relatively small graphs).

\textbf{Working with isomorphism.}
There is also code for working with isomorphisms of graphs, which is used in the search algorithm to shrink the search space further. This is implemented as a module where permutations and operations on these permutations are defined, as well as having the ability to apply a permutation to the nodes of a graph, to get a new but isomorphic graph.

\textbf{Generating all $P_4$-free graphs.}
The library also has a function that allows all \(P_4\)-free graphs of a certain size to be generated. The naive algorithm for doing this which simply generates all graphs and checks each one for being \(P_4\)-free is computationally infeasible for graphs with more than a few variables, as the number of graphs scales superexponentially with the number of variables (for instance there are \(2^{21}\) 7-variable relation webs).
Instead, we use a recursive algorithm that generates all \(P_4\)-free graphs of size \(n\) by first generating the \(P_4\)-free graphs of size \(n-1\) and then checking all possible extensions of these graphs to see if they are \(P_4\)-free. Correctness of this procedure is due to the induced subgraphs of a \(P_4\)-free graph being \(P_4\)-free themselves. In fact, a further optimisation is also added: when we check whether the extensions are \(P_4\)-free, it is sufficient to only check if subsets of the nodes containing the added node are not isomorphic to \(P_4\), instead of checking every subset.

\textbf{Sanity checks.}
Finally, the library also contains some automated tests used as sanity checks on the code, which may be used to check various implementations against each other.

\subsection{Search algorithm}
\label{sec:search}

The main part of the implementation is a search algorithm to find logically minimal non-trivial inferences between relation webs that are not derivable from switch and medial. The search algorithm functions in multiple phases. After each phase the results are serialised and saved to disk so that the algorithm can be restarted from this point.

\textbf{Phase 1: generating $P_4$-free graphs on $n$ nodes.}
Suppose we are searching for $\{\s,\m \}$-independent linear inferences between webs on \(n\) variables. The first phase, as described in the previous section, is to gather all \(P_4\)-free graphs with \(n\) nodes.

\textbf{Phase 2: identifying isomorphism classes and canonical representatives.}
To describe the second phase we need to introduce some new notions. Without loss of generality, we will assume henceforth that the variable set is given by \(\V = \{0,\dots,n-1\}\).

\begin{definition}\label{def:num-rep}
  There is a bijection between \(\{(x,y) \in \mathbb{N} \times \mathbb{N}\ |\ x < y\}\) and \(\mathbb{N}\) given by \(\iota(x,y) = x + \sum_{i < y} i\). Define the \textbf{numerical representation} of a linear graph \(\R\), written \(N(\R)\), to be the natural number whose \(\iota(x,y)\)\textsuperscript{th} least significant bit is \(1\) if and only if \((x,y) \in \R\).
\end{definition}

This is the encoding used to store graph in integers as described in the previous section.

\begin{definition}
Given a bijection $\rho: \V\to \V$, we write $\rho(\R)$ for the graph on $\V$ with edges $(\rho(x),\rho(y))$ for each edge $(x,y) \in \R$.
$\R$ and $\S$ are \textbf{isomorphic} if $\S = \rho(\R)$, for some bijection $\rho: \V\to \V$, in which case $\rho$ is called an \textbf{isomorphism} from $\R $ to $\S$.
\end{definition}

As isomorphism is an equivalence relation, we can partition the set of \(P_4\)-free graphs into isomorphism classes. It can readily be checked that \(N\) (from \cref{def:num-rep}) is injective and can therefore be used to induce a strict total ordering on graphs. Say that a relation web is \textbf{least} if it is the smallest element in its isomorphism class (with respect to this ordering induced from \(N\)).

The second phase of the algorithm is to identify these least relation webs, as well as identify the isomorphism between every relation web and its isomorphic least relation web. It will become clear why this data is needed later on in the section. To obtain this, first the relation webs are sorted (by numerical representation) and then, taking each graph \(\R\) in turn, applying every possible permutation to its nodes, and seeing if any result in a smaller web (with respect to $N$). If none do then we record it as a least relation web (with the identity isomorphism). Otherwise suppose it is isomorphic to \(\R'\) with isomorphism \(\rho\) where \(N(\R') < N(\R)\). As we are checking graphs in order, we must already know that \(\R'\) is isomorphic to least graph \(\R''\) with isomorphism \(\pi\). Then we can record \(\R\) as being isomorphic to \(\R''\) with isomorphism \(\pi \circ \rho\). This allows us to use the following lemma.

\begin{lemma}
  \label{lem:least}
  To prove \cref{thm:main-reduced}, it is sufficient to prove that \(\R \redms \S\) only when \(\R\) is least and \(\R \redms \S\) is logically minimal.
\end{lemma}
\begin{proof}
  By \cref{rem:using-webs}, suppose \(\R\) and \(\S\) are relation webs on \(n\) variables and suppose \(\R \to \S\) is a non-trivial linear inference. By \cref{lem:minimality}, we can further assume that \(\R \to \S\) is logically minimal. Then let \(\rho \) be an isomorphism from $\R$ to \(\R'\) least isomorphic to $\R$, and let \(\S' = \rho(\S)\). Then \(\R \redms \S\) if and only if \(\R' \redms \S'\), as required.
%  where \(\R' \to \S'\) is non-trivial and logically minimal.
\end{proof}
\todo{Anupam: I think I prefer to be more strict about $\redms$ etc. being relations on formulae, not webs.}

The above lemma allows us to only search inferences from least webs to arbitrary webs. This increases the speed of the search greatly as it turns out there are relatively few least formulae. For example, there are \(78416\) \(P_4\)-free graphs with 7 variables with only \(180\) of them being least (the number of isomorphism classes). Note that we may not similarly restrict the RHS of inferences to least webs. This means we need to know the maximal cliques of every \(P_4\)-free graph to determine whether there are inferences between them.

\textbf{Phase 3: generating all maximal cliques.}
In phase three we generate all these maximal cliques and store them so that they do not need to be recomputed every time we check a linear inference. As we can store each maximal clique in a single byte, storing all this data is feasible.

\textbf{Phase 4: generating `least' linear inferences.}
With the maximal clique data, phase four of generating a list of all valid linear inferences (from a least web to an arbitrary web) can be easily done by iterating through all possible combinations and checking them using \cref{lem:rw-inference}.

\textbf{Phase 5: checking for non-triviality}
Similarly phase five of checking which of these inferences are non-trivial is also simple using \cref{lem:rw-trivial}. This data is stored in a \texttt{HashMap} of sets for quick indexing.

\textbf{Phase 6: restricting to logically minimal inferences}
Phase six is now to restrict our inferences to only those that are logically minimal. Write \(\Phi_\R \) be the set of webs distinct from $\R$ that \(\R \) (non-trivially) implies.
We calculate, for a least web $\R$, the set $M_\R$ of webs $\S$ with $\R \to \S$ a logically minimal linear inference using the identity:
\begin{equation*}
  M_\R = \Phi_\R \setminus \bigcup_{\R' \in \Phi_{\R}} \Phi_{\R'}
\end{equation*}
Note that to calculate this, we need to be able to generate \(\Phi_\R\) for arbitrary (i.e.\ not necessarily least) webs. This is where the isomorphism data stored in phase two becomes useful, as if \(\rho \) is an isomorphism from $\R$ to $\R'$, with \(\R'\) least, we can use,
\begin{equation*}
  \Phi_\R = \{ \rho^{-1}(\S) \ |\  \S \in \Phi_{\R'}\}
\end{equation*}
to generate \(\Phi_\R\), where we already have \(\Phi_{\R'}\). In the implementation, we generate each \(\Phi_\R\) on the fly (from \(\Phi_{\R'}\)), though we could have pre-generated all of these, which might provide further speedup for this phase.
%which could provide a significant speedup to this phase.

\textbf{Phase 7: checking for switch-medial derivability.}
The last phase is to check the remaining inferences, of which there are now few enough to feasibly do so. Logically minimal inferences have one further benefit: a logically minimal inference (and in fact any \(\{\s,\m\}\)-minimal inference) \(\phi \to \psi\) is derivable from switch and medial if and only if is derivable from a \emph{single} switch or medial step. To check if it is a medial we can use the criterion for medial derivability from \cref{prop:medial-char}.

To check if the inference \(\R \to \S\) is a switch, we simply run through all possible cograph decompositions of \(\R\) and check if any of the possible switch applications yields \(\S\).
It would have been possible to use the criterion for switch derivability from \cite{Str07:char-med} (mentioned at the end of \cref{sec:webs}), but running through possible partitions of the nodes of \(\R\) was fast enough and easier to implement.

\textbf{Evaluation and main results.}
After running all phases on 7 variables, we found that there were 78416 \(P_4\)-free graphs of which \(180\) were least. There were \(35110\) non-trivial inferences from a least web to an arbitrary web of which \(1352\) were minimal. Of these minimal inferences, \(968\) were an instance of switch, \(384\) were an instance of medial, and there were no other inferences, which completes the proof of \cref{thm:main-reduced}.

Furthermore, the algorithm was fast enough to run on 8 variables, where there were 1320064 \(P_4\)-free graphs of which \(522\) were least. There were \(514486\) non-trivial inferences from a least web to an arbitrary web of which \(5364\) were minimal, Of these, \(3506\) were an instance of switch, \(1770\) were an instance of medial, and there were \(88\) other inferences. After quotienting out by isomorphism (as restricting to inferences from least graphs does not rule out self isomorphisms on the LHS of the inference), we were left with \(3\) inferences, of which two were dual to each other leaving the logically minimal $\{\s,\m\}$-independent inferences given in \cref{sec:8var-inf}. These give a proof of \cref{thm:main}, the main theorem of this paper.



\section{Conclusions}
\label{sec:conclusions}
In this work we undertook a computational approach towards the classification of linear inferences.
To this end we succeeded in exhausting the linear inferences up to 8 variables, showing that there are two (distinct) 8 variable linear inferences that are independent of switch and medial.
One of these new inferences contradicts a conjecture from \cite[Conjecture 7.9]{DasStr16:no-compl-lin-sys}.
Conversely, all linear inferences on 7 variables or fewer are already derivable using switch and medial.


We point out that it should be possible to adapt our implementation to a variety of logics and, in particular, graph-based systems such as those from \cite{AccHorStr20:mll-graphs-short,AccHorStr20:mll-graphs-full,CalDasWar20:bgl}.
This would be an interesting avenue for future work.

%As we mentioned at the start, linear inferences play an important role in substructural logics and their models, as well as structural proof theory more widely (e.g.\ in deep inference).
%Indeed, the particular linear inferences $\s$ and $\m$ are fundamental in that they induce the multiplicatives and the additives, respectively, of linear logic.
%At least one self-contained reason to investigate further linear inferences is to examine whether further `natural' connective types may be obtained for substructural logics, e.g.\ that behave well with cut-elimination and lead to more refined computational interpretations.
%This would be an interesting avenue for future work. \todo{revise this paragraph}
\todo{Anupam: haven't mentioned 9 variables consciously.}

\bibliography{citations}

\appendix

\section{Further proofs and examples}
\label{sect:app:further-proofs-examples}

\begin{proof}
[Proof sketch of \cref{prop:unit-free}]
Write $\leadsto$ for the rewriting relation obtained by orienting every pair of \eqref{eq:un-eq-rel} left-to-right.
Clearly $\leadsto$ is terminating since each step decreases formula size.
For confluence, note that every critical pair\todo{not clear what this means}\todo{Anupam: this is standard rewriting terminology for confluence arguments.} must reduce to the same constant:
\begin{equation*}
\begin{alignedat}{4}
&\bot \lor \bot \leadsto \bot &\quad& \bot \lor \top \leadsto \top &\quad& \top \lor \bot \leadsto \top &\quad& \top \lor \top \leadsto \top \\
&\bot \land \bot \leadsto \bot && \bot \land \top \leadsto \bot && \top \land \bot \leadsto \bot && \top \land \top \leadsto \top
\end{alignedat}\qedhere
\end{equation*}
\end{proof}

\subsection{Recovering an 8 variable inference}
\label{sect:reduction-10var-to-8var}
The reason for writing the variation \eqref{eq:10varinf-nonminimal} in \cref{sec:prev-lin-infs} instead of the one originally presented in \cite{Das13:lin-inf-rew} is that it allows us to recover one of the new 8-variable inferences, by a particular reduction first noticed in a blog post \cite{Das20:lin-inf-size-8}.

By setting $x' = u' = \lnot u$ in \eqref{eq:10varinf-nonminimal} and simplifying, we obtain the linear inference:
\begin{equation*}
\begin{alignedat}{2}
&&& (z \lor (w \land w')) \land (y \lor y') \land ((x \land x') \lor z') \\
&\to &\quad& (z \land (x \lor y)) \lor (w' \land x') \lor ((w \lor y') \land z')
\end{alignedat}
\end{equation*}
%Soundness follows directly from that of \eqref{eq:10varinf-nonminimal}, but independence from $\{\s,\m\}$ needs to be proved again.
Again, the inference above is not $\{\s,\m\}$-minimal, since there are two possible applications of switch to the LHS that nonetheless imply the RHS:
\[
((z \land (y \lor y')) \lor (w \land w')) \land ((x \land x') \lor z')
\quad
\text{or}
\quad
(z \lor (w \land w')) \land ((x \land x') \lor ((y \lor y') \land z'))
\]
Furthermore, are two switch applications leading to the RHS that are nonetheless implied by their respective formulae above:\footnote{Note that these switch applications were overlooked in the blog post \cite{Das20:lin-inf-size-8}.}
\[
((z \lor (w' \land x')) \land (x \lor y)) \lor ((w \lor y') \land z')
\quad
\text{or}
\quad
(z \land (x \lor y)) \lor ((w \lor y') \land ((w' \land x') \lor z'))
\]
The two resulting linear inferences are, in fact, isomorphic and indeed $\{\s,\m\}$-minimal, as we shall explain in the next subsection.
%
As we have already mentioned, the fact that this is a \emph{logically minimal} linear inference is shown by means of the implementation presented in \cref{sec:algorithm}.

\subsection{Validity of \cref{eq:php32-derived-inf-repeated}}
\label{sec:app:validity-php32-derived}
We consider each assignment that satisfies the LHS and argue that it also satisfies the RHS:
\begin{itemize}
\item $\{z,x,x'\}$ satisfies $z\land (x \lor y)$.
\item $\{z,y,z'\}$ satisfies $z\land (x \lor y)$.
\item $\{z,y',z'\}$ satisfies $(w \lor y') \land ((w' \land x') \lor z')$.
\item $\{w, w', x, x'\}$ satisfies $(w \lor y') \land ((w' \land x') \lor z')$.
\item $\{w, w', y, z'\}$ and $\{w, w', y', z'\}$ satisfy $(w \lor y') \land ((w' \land x') \lor z')$.
\end{itemize}

\subsection{Validity of \cref{eq:counterexample-inference-repeated}}
\label{sec:app:validity-counterexample-inference}
We consider each assignment that satisfies the LHS and argue that it also satisfies the RHS:
\begin{itemize}
\item $\{w,w',y,y' \}$ satisfies $w\land y$.
\item $\{w,w',z,z' \}$ satisfies $w' \land z' $ and $z$.
\item $\{ x,x',y,y' \}$ satisfies $x$ and $x' \land y' $.
\item $\{x,x',z,z' \}$ satisfies $x$ and $z$.
\end{itemize}

\subsection{$\{\s,\m\}$-independence and $\{\s,\m\}$-minimality of \cref{eq:php32-derived-inf-repeated}}
\label{sec:ind-min-php32}

There are two possible medial applications to the subformula $(x \land x') \lor ((y \lor y') \land z')$ resulting in the following new LHSs:
\begin{itemize}
\item $(z \lor (w \land w')) \land (x \lor y \lor y') \land (x' \lor z')$. In this case $\{z, y', x'\}$ is a countermodel.
\item $(z \lor (w \land w')) \land (x \lor z') \land (x' \lor y \lor y')$. In this case $\{z, z', x'\}$ is a countermodel.
\end{itemize}

There are two possible switch applications to the subformula $(y \lor y') \land z'$ resulting in the following new LHSs:
\begin{itemize}
\item $(z \lor (w \land w')) \land ((x \land x') \lor y \lor (y' \land z'))$. In this case $\{w ,w',y\}$ is a countermodel.
\item $(z \lor (w \land w')) \land ((x \land x') \lor y' \lor (y \land z'))$. In this case $\{z, y'\}$ is a countermodel.
\end{itemize}

Finally any other switch application is on the top-level conjunction, resulting in a formula of the form $z \lor X$, $(w \land w') \lor X$, $(x \land x') \lor X$ or $((y \lor y') \land z') \lor X$, which admits a countermodel $\{z\}$, $\{w, w'\}$, $\{x, x'\}$ or $\{y ,z'\}$, respectively.

\subsection{$\{\s,\m \}$-independence and $\{\s,\m \}$-minimality of \cref{eq:counterexample-inference-repeated}}
\label{sec:ind-min-counterexample-inference}
Let us first consider rules applicable to the LHS.
There are four possible medial applications, resulting in the following new LHSs:
\begin{itemize}
\item $(w \lor x) \land (w' \lor x' ) \land ((y \land y') \lor (z \land z'))$. In this case $\{w,x',y,y' \}$ is a countermodel.
\item $(w \lor x') \land (w' \lor x) \land ((y \land y') \lor (z \land z'))$. In this case $\{x',w',y, y' \}$ is a countermodel.
\item $((w \land w') \lor (x \land x')) \land (y \lor z) \land (y' \lor z' )$. In this case $\{ x,x',y,z' \}$ is a countermodel.
\item $((w \land w') \lor (x \land x')) \land (y \lor z' ) \land (y' \lor z)$. In this case $\{ w,w',z', y'  \}$ is a countermodel.
\end{itemize}
Any switch application to the LHS must be on the top-level conjunction, and will have the form $(a \land a') \lor X$, for $a \in \{w,x,y,z \}$. However, $\{w,w' \}$, $\{x,x' \}$, $\{y,y' \}$ and $\{z,z'\}$ are each countermodels for the RHS.

Now let us consider the possible rule applications leading to the RHS.
There are two possible medial instances, coming from the following new RHSs:
\begin{itemize}
\item $(w \land y) \lor (x \land x' \land y') \lor (w' \land z' \land z)$. In this case $\{x,x',z,z'\}$ is a countermodel.
\item $(w \land y) \lor (x \land z) \lor (w' \land z' \land x' \land y')$. In this case $\{ w,w',z,z' \}$ is a countermodel.
\end{itemize}
Now let us consider the switch instances:
\begin{itemize}
\item If the contractum of the switch is $x \lor (w' \land z')$, then $\{x,x',y,y'\}$ is a countermodel.
\item If the contractum of the switch is $(x' \land y') \lor z$, then $\{w,w',z,z'\}$ is a countermodel.
\item If the redex of the switch has the form $w \land X$ or $y \land X$, then $\{x,x',z,z'\}$ is a countermodel.
\item If the redex of the switch has the form $X \land (x \lor (w' \land z'))$ or $X \land ((x' \land y') \lor z)$, then $\{ w,w',y,y' \}$ is a countermodel.
\end{itemize}



\end{document}
